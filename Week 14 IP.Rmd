---
title: "Week 14 IP"
author: "Ted Askoye"
date: "9/23/2020"
output: pdf_document
---
# 1. Business Understanding
## 1 a.) Defining the Question 
#### You are a Data analyst at Carrefour Kenya and are currently undertaking a project that will inform the marketing department on the most relevant marketing strategies that will result in the highest no. of sales (total price including tax). Hence making an analysis to Customer Data from a the supermarket and implement dimensionality reduction.

# 2. Defining the Metrics of Success
#### The success of this analysis will occur when we analysis the customer data to understand it fully and later implementing the appropriate dimensionality reduction techniques.

# 3. Context

##### Dimensionality reduction is the process of reducing the number of random variables under review, by getting a set of principal variables. It can be divided into feature selection and feature extraction and is important for the visualization of features while it also helps deal with multicollinearity of the features.


# 4. Experimental Design

#### We will define the question, the metric of success, context and experimental design taken. This will be followed by reading and exploring the dataset and its appropriateness of the available data to answer the given question. This will be followed by cleaning the data off outliers, anomalies and null values from missing data, perfom an exploratory data analysis after which we will Implement feature extraction and feature selection,record our observations and provide a conclusion and reccomendation.


# 5. Data Relevance

### Our data is very relevant to our research question.


# 6. Loading relevant Libraries and Reading the Data
```{r}
# Importing the required packages

library("data.table")
library("plyr")
library("dplyr")
library("tidyverse")
library("tidyr")
library("lubridate")
library("ggcorrplot")
library("ggplot2")
library("corrplot")
library("moments")
library('xtable')
library('countrycode')
library('class')
library("rpart")
library("rpart.plot")
library("mlbench")
library('e1071')
library('rpart')
library('caret')
library('ranger')
library('kernlab')
library('ggbiplot')
library('ISLR')
library('devtools')



# Loading the Dataset

cf_df <- read.csv(url("http://bit.ly/CarreFourDataset"))

```

## Previewing the data 
 
```{r}
# Previewing The First Seven records in the Dataset

head(cf_df, n=7)

```

```{r}

# Previewing The Last Seven records in the Dataset

tail(cf_df, n=7)
```

```{r}
# Checking the Data Dimensions

dim(cf_df)
```

### The dataset has 1000 records and 10 columns

```{r}
# Checking the Structure of the Dataset

str(cf_df)
```

```{r}
# Checking The Data present in each column

glimpse(cf_df)

```

# 7. Data Preparation

## Uniformity

```{r}
# Check column names

colnames(cf_df)
```
#### We'll rename the column names for Uniformity purposes

```{r}
# Renaming column names

names(cf_df)[1]<- 'Invoice_ID'
names(cf_df)[3]<- 'Customer_type'
names(cf_df)[5]<- 'Product_line'
names(cf_df)[6]<- 'Unit_price'
names(cf_df)[14]<- 'Gross_income'

# Checking whether the column names have been changed

colnames(cf_df)
```

```{r}
# Checking for the length of unique values in each column

lapply(cf_df, function (x) {length(unique(x))})

```
```{r}

# Cheking if Tax and gross columns are duplicated

unique(cf_df$Tax == cf_df$Gross_income)
```


#### Gross income percentage' has one unique variable making it redundant in our analysis.
```{r}
# Drop the Gross Margin percentage and Tax (Tax and Gross income are duplicated ) column

cf_df <- cf_df[, -8]
cf_df <- cf_df[, -12]

dim(cf_df)

```

## Completeness

```{r}
# Checking for missing values

colSums(is.na(cf_df))
```

```{r}
# Checking for duplicate values

duplicates <- cf_df[duplicated(cf_df),]
duplicates
```

### Outlier Detection
#### # Checking for anomalies in our numerical variables 

```{r}
# Plotting boxplots for all the numerical variables

par(mfrow=c(3,2))
boxplot((cf_df$`Unit_price`), horizontal = TRUE, col = 'red', main = "Unit_price")
boxplot((cf_df$`Quantity`), horizontal = TRUE, col = 'blue', main = "Quantity")
boxplot((cf_df$`Gross`), horizontal = TRUE, col = 'green', main = "Gross_income")
boxplot((cf_df$`cogs`), horizontal = TRUE, col = 'orange', main = "cogs")
boxplot((cf_df$`Total`), horizontal = TRUE, col = 'purple', main = "Total")
boxplot((cf_df$`Rating`), horizontal = TRUE, col = 'skyblue', main = "Rating")

```
#### We will not remove the outliers as they may have vital information

# 8. Exploratory Data Analysis

## Univariate Analysis

```{r}

# Checking the statistical summary of the data

summary(cf_df)

```

## Measures of Central Tendancy and Dispersion -  Summary

###  Central Tendancy - Mode, Mean and Median
```{r}
# First, a function for mode will be created since R does not have a built in function.

getmode <- function(v) {
    uniqv <- unique(v) 
    uniqv[which.max(tabulate(match(v, uniqv)))]
}
```

```{r}
# Unit Price
mode.up <- getmode(cf_df$Unit_price) 
mode.up
mean(cf_df$Unit_price)
median(cf_df$Unit_price)
```
```{r}
# Gross_income 

mode.gi <- getmode(cf_df$Gross_income)  
mode.gi
mean(cf_df$Gross_income)
median(cf_df$Gross_income)
```

```{r}
# Quantity

mode.quan <- getmode(cf_df$Quantity)  
mode.quan
mean(cf_df$Quantity)
median(cf_df$Quantity)
```


```{r}
# Cogs

mode.cogs <- getmode(cf_df$cogs) 
mode.cogs
mean(cf_df$cogs)
median(cf_df$cogs)
```

```{r}
# Total
mode.total <- getmode(cf_df$Total) 
mode.total
mean(cf_df$Total)
median(cf_df$Total)
```
```{r}
# Rating
mode.rating <- getmode(cf_df$Rating) 
mode.rating
mean(cf_df$Rating)
median(cf_df$Rating)
```



#### Measure of Dispersion and Histograms - Standard Deviation, Variance, Skewness, Kurtosis and Range

```{r}
# Unit price

hist((cf_df$`Unit_price`), col = 'orange', main = "Unit Price")

sd.up <- sd(cf_df$Unit_price)
sd.up

var.up <- var(cf_df$Unit_price)
var.up

range.up <- range(cf_df$Unit_price)
range.up

skew.up <- skewness(cf_df$Unit_price)
skew.up

kurt.up <- kurtosis(cf_df$Unit_price)
kurt.up
```


```{r}
# Gross income

hist((cf_df$`Gross_income`), col = 'orange', main = "Gross Income")

sd.gi <- sd(cf_df$Gross_income)
sd.gi

var.gi <- var(cf_df$Gross_income)
var.gi

range.gi <- range(cf_df$Gross_income)
range.gi

skew.gi <- skewness(cf_df$Gross_income)
skew.gi

kurt.gi <- kurtosis(cf_df$Gross_income)
kurt.gi
```

```{r}
# Quantity


hist((cf_df$`Quantity`), col = 'orange', main = "Quantity")

sd.quan <- sd(cf_df$Quantity)
sd.quan

var.quan <- var(cf_df$Quantity)
var.quan

range.quan <- range(cf_df$Quantity)
range.quan

skew.quan <- skewness(cf_df$Quantity)
skew.quan

kurt.quan <- kurtosis(cf_df$Quantity)
kurt.quan
```

```{r}
# Cogs

hist((cf_df$`cogs`), col = 'orange', main = "Cogs")

sd.cogs <- sd(cf_df$cogs)
sd.cogs

var.cogs <- var(cf_df$cogs)
var.cogs

range.cogs <- range(cf_df$cogs)
range.cogs

skew.cogs <- skewness(cf_df$cogs)
skew.cogs

kurt.cogs <- kurtosis(cf_df$cogs)
kurt.cogs
```

```{r}
# Total

hist((cf_df$`Total`), col = 'orange', main = "Total")

sd.total <- sd(cf_df$Total)
sd.total

var.total <- var(cf_df$Total)
var.total

range.total <- range(cf_df$Total)
range.total

skew.total <- skewness(cf_df$Total)
skew.total

kurt.total <- kurtosis(cf_df$Total)
kurt.total
```
```{r}
# Rating

hist((cf_df$`Rating`), col = 'orange', main = "Rating")

sd.r <- sd(cf_df$Rating)
sd.r

var.r <- var(cf_df$Rating)
var.r

range.r <- range(cf_df$Rating)
range.r

skew.r <- skewness(cf_df$Rating)
skew.r

kurt.r <- kurtosis(cf_df$Rating)
kurt.r
```
### Barplots

```{r}
# Branches
barplot(table(cf_df$Branch), main = "Branches")
```
#### Branch A has the most activity but there is very little difference between them.


```{r}
# Customer Type
barplot(table(cf_df$Customer_type), main = "Customer Type")
```
Transactions between the members and non-members are equally matched.

```{r}
# Gender
barplot(table(cf_df$Gender), main = "Gender")
```
#### There seems to be very little to no difference in terms of Genders

```{r}
# Product Line

barplot(table(cf_df$Product_line), main = "Bar chart of Product Line", las=2)

```
### It seems Electronics and accessories and food and berverages are the most popular product lines.

```{r}
# Payment
barplot(table(cf_df$Payment), main = "Payment")
```
#### Most of the customers transact using their ewallet or using cash.

## Bivariate Analysis

```{r}
# Creating a new dataframe num with numerical data variables
Unit_price<- cf_df$Unit_price  
Gross_income<-cf_df$Gross_income 
cogs<-cf_df$cogs  
Total<-cf_df$Total    
Rating<-cf_df$Rating    

num_data <- data.frame(Unit_price, Gross_income, cogs, Total, Rating)
head(num_data)  
```
```{r}
# Correlation
# Correlation is a statistical technique that can show whether and how strongly pairs of variables are related.

# Calculating the correlation matrix

corr <- cor(num_data)
head(corr)
```
```{r}

# Plotting the correlation matrix

ggcorrplot(corr,hc.order = TRUE)
```
#### We observe that most of the variables are perfectly correlated which is problematic in modelling hence the need for feature extraction or feature selection.

### Scatterplots

```{r}
par(mfrow=c(2,4))
plot(num_data$Gross_income,num_data$Total, main="Total vs. Gross income")
plot(num_data$cogs, num_data$Total, main="Total vs. cogs")
plot(num_data$Unit_price, num_data$Total, main="Total vs. Unit price")
plot(num_data$Unit_price,num_data$cogs, main="Unit price vs. cogs")
plot(num_data$Unit_price,num_data$Gross_income, main="Unit price vs. Gross_income")
plot(num_data$Unit_price,num_data$Total, main="Unit price vs. Total")

```
# 9. Implementing The Solution

### Principal Component Analysis 
####Weâ€™ll perform and visualize PCA in the given dataset.
```{r}
# Selecting the numerical data 
cf_df_num <- select_if(cf_df,is.numeric)
str(cf_df_num)
```

```{r}
# We then pass the data to the prcomp() and set the center and scale arguments, to be FALSE and TRUE 

ef.pca <- prcomp(cf_df_num, center = FALSE, scale. = TRUE)
summary(ef.pca)
```
#### PC1 explains 90% of the total variance, which means that more than three-quarters of the information in the dataset  can be encapsulated by just that one Principal Component. PC2 explains 6.1% of the variance, PC3 - 2.2% and PC4- 0.4% 

```{r}
# Calling str() to have a look at our PCA object
str(ef.pca)
```

```{r}
# We will now plot our pca.
set.seed(123)
ggbiplot(ef.pca, labels=rownames(ef.pca),ellipse = TRUE,obs.scale=1,var.scale=1)
```
#### This is not so clear, weâ€™ll therefore plot to see the number of components that contribute more to PC1

```{r}
plot(ef.pca, type="l")
```
6 components contribute more but PC1 contribte the most in this anaysis.

### Part 2: Feature Selection

```{r}
# Calculating the correlation matrix

corrmatrix <- cor(num_data)

# Find attributes that are highly correlated

highcorr <- findCorrelation(corrmatrix, cutoff=0.75)

# Highly correlated attributes

highcorr

names(num_data[highcorr])
```
#### We observe that "Gross_income" and "cogs" are highly correlated with the other features.

```{r}
# Removing Redundant Features
num_data_clean <- num_data[-highcorr]
# Performing our graphical comparison
par(mfrow = c(1, 2))
corrplot(corrmatrix, order = "hclust")
corrplot(cor(num_data_clean), order = "hclust")
```
### Removing highly correlated variables result to less coreelated variables. Hence the selected features are Unit_Price, Total and Rating. There are no more highly correlated variables.

## Part 3 : Association Rules
##### This section will require that you create association rules that will allow you to identify relationships between variables in the dataset.

```{r}
# Installing and reading the necessary packages for the association rules analysis
#install.packages("arules", dependencies = TRUE)
library(arules)

```
```{r}
# Reading and previewing the dataset as transcations

sdf <- read.transactions("http://bit.ly/SupermarketDatasetII")
head(sdf)

```
```{r}
# Checking the dimensions of the data
dim(sdf)
```
##### The dataset has 7,501 transactions and 5729 columns
```{r}
# Displaying the structure of our dataset

str(sdf)
```
```{r}
# Verifying the class of the object
class(sdf)
```

```{r}
# Previewing the first few items in the dataset
inspect(sdf[1:5])
```
```{r}
# Previewing the items in the dataset as if it were in a dataframe
items<-as.data.frame(itemLabels(sdf))
colnames(items) <- "Item"
head(items, 10)
```
```{r}
# Getting the summary statistics of the data

summary(sdf)
```
```{r}
# Exploring the frequency of some articles

itemFrequency(sdf[, 7:11],type = "absolute")
```


```{r}
# Producing a chart of frequencies and filtering to consider only items with a minimum percentage of support
par(mfrow = c(1, 3))
# plot the frequency of items
itemFrequencyPlot(sdf, topN = 10,col="grey")
itemFrequencyPlot(sdf, support = 0.1,col="pink")
```
# The top 10 most common items in the transactions dataset are Mineral water, eggs, spaghetti, french fries, chocolate, green tea, milk, ground beef, frozen vegetables and pancakes.

```{r}

# Setting the parameters for our association analysis
rules <- apriori (sdf, parameter = list(supp = 0.001, conf = 0.8))
rules
```
```{r}
# Building a apriori model with Min Support as 0.002 and confidence as 0.8.

rules2 <- apriori (sdf,parameter = list(supp = 0.002, conf = 0.8)) 
rules2
# Building apriori model with Min Support as 0.002 and confidence as 0.6.

rules3 <- apriori (sdf, parameter = list(supp = 0.001, conf = 0.6)) 

rules3
```
```{r}
#Performing an exploration of our model using the summary function
summary(rules)
```
```{r}
# Observing rules built in our model i.e. first 10 model rules

inspect(rules[1:10])
```
```{r}
# Ordering these rules by a criteria such as the level of confidence then looking at the first five rules.
rules<-sort(rules, by="confidence", decreasing=TRUE)
inspect(rules[1:5])
```
## Part 4 : Anomaly Detection

#### This is to detect whether there are any anomalies in the given sales dataset. 

```{r}
# Reading and previewing the dataset

ano <- read.csv("http://bit.ly/CarreFourSalesDataset")
head(ano)
```
```{r}
# Checking the dimensions of our dataset

dim(ano)
```

```{r}
# Checking the structure of our dataset

str(ano)
```

```{r}
# Checking the Statistical summary

summary(ano)
```

```{r}
# Checking for null values

colSums(is.na(ano))
```

```{r}
# Changing the date column from character to Date

ano <- transform(ano, Date = format(as.Date(Date, '%m/%d/%Y'), '%Y/%m/%d'))
ano <- transform(ano, Date = as.Date(Date))
sapply(ano, class)
```

```{r}
# Grouping the dataset by the Date column

Sales <- ano$Sales
Date <- ano$Date
ano = ano %>% arrange(Date)
head(ano)
```


